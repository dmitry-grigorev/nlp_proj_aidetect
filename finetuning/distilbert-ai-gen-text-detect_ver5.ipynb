{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10268139,"sourceType":"datasetVersion","datasetId":6352740}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torcheval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:40:45.775105Z","iopub.execute_input":"2025-01-06T17:40:45.775375Z","iopub.status.idle":"2025-01-06T17:40:50.507133Z","shell.execute_reply.started":"2025-01-06T17:40:45.775346Z","shell.execute_reply":"2025-01-06T17:40:50.506067Z"}},"outputs":[{"name":"stdout","text":"Collecting torcheval\n  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.12.2)\nDownloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torcheval\nSuccessfully installed torcheval-0.0.7\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom transformers import DistilBertTokenizer, DistilBertModel\n#from datasets import Dataset\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom tqdm import tqdm\nimport pandas as pd\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:40:50.508229Z","iopub.execute_input":"2025-01-06T17:40:50.508486Z","iopub.status.idle":"2025-01-06T17:41:04.754674Z","shell.execute_reply.started":"2025-01-06T17:40:50.508466Z","shell.execute_reply":"2025-01-06T17:41:04.753874Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"torch.random.manual_seed(0)\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:41:04.755537Z","iopub.execute_input":"2025-01-06T17:41:04.756019Z","iopub.status.idle":"2025-01-06T17:41:07.317868Z","shell.execute_reply.started":"2025-01-06T17:41:04.755999Z","shell.execute_reply":"2025-01-06T17:41:07.317233Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16a60fa1e37e40b796747609a96fe4c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4199e79f0ca84b81930ab6fdc988bb30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4249eaf5d244b4cbf980f48eef9afe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4bc2c22844440e4a2acbdfbb88c08db"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad08830d451a409bb7c450927e6f83ce"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:41:07.318651Z","iopub.execute_input":"2025-01-06T17:41:07.318856Z","iopub.status.idle":"2025-01-06T17:41:07.324082Z","shell.execute_reply.started":"2025-01-06T17:41:07.318838Z","shell.execute_reply":"2025-01-06T17:41:07.323419Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): GELUActivation()\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\nn1, n2 = get_n_params(model.embeddings), get_n_params(model.transformer)\nn1, n2, n1+n2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:41:07.326442Z","iopub.execute_input":"2025-01-06T17:41:07.326688Z","iopub.status.idle":"2025-01-06T17:41:07.340228Z","shell.execute_reply.started":"2025-01-06T17:41:07.326669Z","shell.execute_reply":"2025-01-06T17:41:07.339584Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(23835648, 42527232, 66362880)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_test_data = pd.read_csv('/kaggle/input/human-vs-qwen25-n-phi3/train_test_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:41:07.341200Z","iopub.execute_input":"2025-01-06T17:41:07.341506Z","iopub.status.idle":"2025-01-06T17:41:08.983900Z","shell.execute_reply.started":"2025-01-06T17:41:07.341487Z","shell.execute_reply":"2025-01-06T17:41:08.983205Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\n\n\ndef enable_determinism():\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n    torch.use_deterministic_algorithms(True)\n\ndef fix_seeds(seed: int):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.mps.manual_seed(seed)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\nenable_determinism()\nfix_seeds(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:41:08.984721Z","iopub.execute_input":"2025-01-06T17:41:08.985032Z","iopub.status.idle":"2025-01-06T17:41:08.994441Z","shell.execute_reply.started":"2025-01-06T17:41:08.985002Z","shell.execute_reply":"2025-01-06T17:41:08.993547Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Класс датасета\nclass AnswersDataset(Dataset):\n  def __init__(self, tokenizer, data_df, sampletype,  max_len=512):\n    self.raw_data = data_df[data_df['sample_type']==sampletype]\n\n    self.max_len = max_len\n    self.tokenizer = tokenizer\n    self.inputs_q = []\n    self.inputs_a = []\n    self.masks_q = []\n    self.masks_a = []\n    self.targets = []\n\n    self.class_mapper = {'Human': 0, 'Phi3-mini': 1, 'Qwen25': 2}\n\n    self.class_mapper_inv = {v: k for k, v in self.class_mapper.items()}\n\n    self._build()\n\n\n  def __len__(self):\n    return len(self.inputs_a)\n\n  def __getitem__(self, index):\n    question_ids = self.inputs_q[index].squeeze()\n    answers_ids = self.inputs_a[index].squeeze()\n\n    question_mask = self.masks_q[index].squeeze()\n    answer_mask = self.masks_a[index].squeeze()\n\n    target_ids = self.targets[index]\n\n    #src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n    #target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n\n    return  question_ids, answers_ids, target_ids, question_mask, answer_mask\n    #{\"question_ids\": question_ids, \"answers_ids\": answers_ids, \"target_ids\": target_ids}\n\n  def _build(self):\n    self._buil_examples_from_files()\n\n  def _buil_examples_from_files(self):\n    # REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()]\")\n    # REPLACE_WITH_SPACE = re.compile(\"()|(\\-)|(\\/)\")\n\n    for i, row in tqdm(self.raw_data.iterrows(), total=self.raw_data.shape[0]):\n\n      if pd.isna(row['Answers']):\n        continue\n\n      text_question = row['Question']\n      text_answer = row['Answers']\n\n      line_question = text_question.strip()\n      line_answer = text_answer.strip()\n\n\n      # line = REPLACE_NO_SPACE.sub(\"\", line)\n      # line = REPLACE_WITH_SPACE.sub(\"\", line)\n      # line = line + ' '\n\n      target = self.class_mapper[row['Author']]\n\n       # tokenize inputs\n      tokenized_questions, q_mask = [v for k, v in self.tokenizer.batch_encode_plus(\n          [line_question], max_length=self.max_len, padding='max_length', return_tensors=\"pt\",\n          truncation=True\n      ).items()]\n      tokenized_answers, a_mask = [v for k, v in self.tokenizer.batch_encode_plus(\n          [line_answer], max_length=self.max_len, padding='max_length', return_tensors=\"pt\",\n          truncation=True\n      ).items()]\n\n       # tokenize targets\n\n\n      self.inputs_q.append(tokenized_questions)\n      self.inputs_a.append(tokenized_answers)\n      self.masks_q.append(q_mask)\n      self.masks_a.append(a_mask)\n      self.targets.append(target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:41:08.995577Z","iopub.execute_input":"2025-01-06T17:41:08.995888Z","iopub.status.idle":"2025-01-06T17:41:09.005898Z","shell.execute_reply.started":"2025-01-06T17:41:08.995855Z","shell.execute_reply":"2025-01-06T17:41:09.005109Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"max_seq_length = 128 # with 256 one epoch with 2 evaluations take 2 hours together\ntrain_dataset = AnswersDataset(tokenizer, train_test_data, 'train', max_len=max_seq_length)\n#test_dataset = AnswersDataset(tokenizer, train_test_data, 'test', max_len=max_seq_length)\nval_dataset = AnswersDataset(tokenizer, train_test_data, 'val', max_len=max_seq_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:41:09.006596Z","iopub.execute_input":"2025-01-06T17:41:09.006834Z","iopub.status.idle":"2025-01-06T17:45:36.296714Z","shell.execute_reply.started":"2025-01-06T17:41:09.006815Z","shell.execute_reply":"2025-01-06T17:45:36.295879Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 67699/67699 [03:22<00:00, 334.37it/s]\n100%|██████████| 21233/21233 [01:04<00:00, 327.83it/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"g = torch.Generator()\nbatch_size=128\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g,\n                          pin_memory=True, num_workers=2, worker_init_fn=seed_worker)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n                        pin_memory=True, num_workers=2, worker_init_fn=seed_worker)\n#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:45:36.297750Z","iopub.execute_input":"2025-01-06T17:45:36.298108Z","iopub.status.idle":"2025-01-06T17:45:36.302401Z","shell.execute_reply.started":"2025-01-06T17:45:36.298074Z","shell.execute_reply":"2025-01-06T17:45:36.301581Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn as nn\nclass DistilBERTClf_FCtuned_v5(nn.Module):\n    def __init__(self, max_seq_length=512):\n        super(DistilBERTClf_FCtuned_v5, self).__init__()\n        self.dbmodel = model\n        # v1 one head of additional attention\n        # v2 two heads\n        # v3 three heads\n        # v4 one head, no dropout in the foremost attention\n        # v5 GELU between FC and the attention\n        self.model_out_features =  self.dbmodel.transformer.layer[-1].ffn.lin2.out_features\n        self.attention_layer = nn.MultiheadAttention(embed_dim=self.model_out_features,\n                                                     num_heads=1, dropout=0.1, batch_first=True)\n        self.length=max_seq_length\n        self.fc = nn.Linear(in_features=self.length*self.model_out_features, out_features = 3)\n        self.activation_inner = nn.GELU()\n        self.activation = nn.Softmax(dim=1)\n        self.freeze_layers()\n\n\n    def freeze_layers(self):\n      for param in self.dbmodel.parameters():\n        param.requires_grad = False\n\n\n\n    def forward(self, qx, ax, qmask, amask):\n        qout = self.dbmodel(input_ids=qx, attention_mask=qmask).last_hidden_state\n        aout = self.dbmodel(input_ids=ax, attention_mask=amask).last_hidden_state\n        out, out_weights = self.attention_layer(qout, aout, aout)\n        out = out.reshape(out.shape[0], -1) \n        out = self.activation_inner(out)\n        pred = self.fc(out)\n\n        return self.activation(pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:45:36.306175Z","iopub.execute_input":"2025-01-06T17:45:36.306378Z","iopub.status.idle":"2025-01-06T17:45:36.315166Z","shell.execute_reply.started":"2025-01-06T17:45:36.306360Z","shell.execute_reply":"2025-01-06T17:45:36.314460Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from torcheval.metrics.functional import multiclass_confusion_matrix as conf_matrix\n\ndef evaluation_epoch(model, loader, loss_obj):\n  n_correct, n_total = 0, 0\n  n_correct_oo = 0\n  total_loss = 0\n\n  total_conf_matrix = torch.zeros(size=(3, 3))\n\n  with torch.no_grad():\n    for chunk in tqdm(loader):\n      qx, ax, y, qmask, amask = chunk\n      pred = model(qx.to(device), ax.to(device), qmask.to(device), amask.to(device)).cpu()\n      apred = torch.argmax(pred, 1) # for all-vs-all classification\n\n      oopred = torch.where(apred > 0, 1, 0) # for one-vs-others classification\n      ooy = torch.where(y > 0, 1, 0)\n\n      n_correct += (apred == y).sum()\n      n_correct_oo += (oopred == ooy).sum()\n\n      n_total += y.shape[0]\n\n      total_loss += y.shape[0] * loss_obj(pred, y).item()\n      total_conf_matrix+=conf_matrix(apred, y, num_classes=3)\n\n  return {'accuracy_ava': n_correct/n_total, 'loss': total_loss/n_total,\n          'accuracy_ovo':n_correct_oo/n_total, 'conf_matrix': total_conf_matrix}\n\ndef train_neural_net(model, train_loader, test_loader):\n  loss = nn.CrossEntropyLoss()\n  optimizer=torch.optim.Adam([{'params': model.attention_layer.parameters()},\n                              {'params': model.fc.parameters()}], lr=1e-4)\n  n_epochs=8\n  loss_train_history = [] # логируется всегда\n\n  # логируются каждую эпоху\n\n  train_epoch_evals = []\n  test_epoch_evals = []\n\n  for _ in range(n_epochs):\n    i=0\n    model.train().to(device)\n    for train_chunk in tqdm(train_loader):\n        qx, ax, y, qmask, amask = train_chunk\n        optimizer.zero_grad(set_to_none=True)\n\n\n        pred = model(qx.to(device), ax.to(device), qmask.to(device), amask.to(device))\n        loss_val = loss(pred, y.to(device)) #.long()\n        loss_val.backward()\n\n        loss_val_item = loss_val.detach().cpu().item()\n\n        optimizer.step()\n        loss_train_history.append(loss_val_item)\n        i+=1\n        if i % 100 == 0:\n          print(f'train step {i}: train loss = {loss_val_item :.3f}')\n        #break\n\n    model.eval()\n    #model.cpu()\n\n    #train\n    print('train evaluation')\n    train_eval = evaluation_epoch(model, train_loader, loss)\n    print(f\"epoch {_}: train ava accuracy = {train_eval['accuracy_ava'] :.3f}\")\n    print(f\"epoch {_}: train loss = {train_eval['loss'] :.3f}\")\n    print(f\"epoch {_}: train ovo accuracy = {train_eval['accuracy_ovo'] :.3f}\")\n    train_epoch_evals.append(train_eval)\n\n    #test\n    print('test evaluation')\n    test_eval = evaluation_epoch(model, test_loader, loss)\n    print(f\"epoch {_}: test ava accuracy = {test_eval['accuracy_ava'] :.3f}\")\n    print(f\"epoch {_}: test loss = {test_eval['loss'] :.3f}\")\n    print(f\"epoch {_}: test ovo accuracy = {test_eval['accuracy_ovo'] :.3f}\")\n    test_epoch_evals.append(test_eval)\n\n  return {'training_loss_history': loss_train_history,\n          'train_epochs_res': train_epoch_evals,\n          'test_epochs_res': test_epoch_evals\n          }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:45:36.316100Z","iopub.execute_input":"2025-01-06T17:45:36.316403Z","iopub.status.idle":"2025-01-06T17:45:37.022208Z","shell.execute_reply.started":"2025-01-06T17:45:36.316378Z","shell.execute_reply":"2025-01-06T17:45:37.021573Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"enable_determinism()\nfix_seeds(0)\n\nmodel1 = DistilBERTClf_FCtuned_v5(max_seq_length)\ntrain_hist = train_neural_net(model1, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:45:37.022980Z","iopub.execute_input":"2025-01-06T17:45:37.023276Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/529 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n 19%|█▉        | 100/529 [01:50<08:05,  1.13s/it]","output_type":"stream"},{"name":"stdout","text":"train step 100: train loss = 0.828\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 200/529 [03:45<06:19,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 200: train loss = 0.777\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 300/529 [05:41<04:27,  1.17s/it]","output_type":"stream"},{"name":"stdout","text":"train step 300: train loss = 0.830\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 400/529 [07:37<02:29,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 400: train loss = 0.771\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▍| 500/529 [09:33<00:33,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 500: train loss = 0.799\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 529/529 [10:07<00:00,  1.15s/it]\n","output_type":"stream"},{"name":"stdout","text":"train evaluation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 529/529 [09:35<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch 0: train ava accuracy = 0.776\nepoch 0: train loss = 0.769\nepoch 0: train ovo accuracy = 0.908\ntest evaluation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 166/166 [03:00<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch 0: test ava accuracy = 0.756\nepoch 0: test loss = 0.787\nepoch 0: test ovo accuracy = 0.907\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 100/529 [01:55<08:14,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 100: train loss = 0.719\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 200/529 [03:50<06:18,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 200: train loss = 0.787\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 300/529 [05:45<04:24,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 300: train loss = 0.753\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 400/529 [07:40<02:28,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 400: train loss = 0.775\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▍| 500/529 [09:36<00:33,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 500: train loss = 0.781\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 529/529 [10:09<00:00,  1.15s/it]\n","output_type":"stream"},{"name":"stdout","text":"train evaluation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 529/529 [09:38<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch 1: train ava accuracy = 0.793\nepoch 1: train loss = 0.753\nepoch 1: train ovo accuracy = 0.916\ntest evaluation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 166/166 [03:00<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch 1: test ava accuracy = 0.766\nepoch 1: test loss = 0.778\nepoch 1: test ovo accuracy = 0.911\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 100/529 [01:55<08:18,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 100: train loss = 0.743\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 200/529 [03:51<06:22,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 200: train loss = 0.713\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 300/529 [05:48<04:25,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 300: train loss = 0.750\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 400/529 [07:43<02:28,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 400: train loss = 0.780\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▍| 500/529 [09:38<00:33,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 500: train loss = 0.698\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 529/529 [10:11<00:00,  1.16s/it]\n","output_type":"stream"},{"name":"stdout","text":"train evaluation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 529/529 [09:36<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch 2: train ava accuracy = 0.810\nepoch 2: train loss = 0.736\nepoch 2: train ovo accuracy = 0.914\ntest evaluation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 166/166 [03:01<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch 2: test ava accuracy = 0.798\nepoch 2: test loss = 0.748\nepoch 2: test ovo accuracy = 0.910\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 100/529 [01:56<08:17,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 100: train loss = 0.723\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 200/529 [03:51<06:19,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 200: train loss = 0.707\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 300/529 [05:47<04:24,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 300: train loss = 0.717\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 400/529 [07:42<02:28,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 400: train loss = 0.718\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▍| 500/529 [09:37<00:33,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 500: train loss = 0.726\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 529/529 [10:11<00:00,  1.16s/it]\n","output_type":"stream"},{"name":"stdout","text":"train evaluation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 529/529 [09:36<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch 3: train ava accuracy = 0.822\nepoch 3: train loss = 0.725\nepoch 3: train ovo accuracy = 0.929\ntest evaluation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 166/166 [02:59<00:00,  1.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch 3: test ava accuracy = 0.812\nepoch 3: test loss = 0.734\nepoch 3: test ovo accuracy = 0.927\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 100/529 [01:55<08:16,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 100: train loss = 0.695\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 200/529 [03:51<06:22,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 200: train loss = 0.734\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 300/529 [05:48<04:25,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"train step 300: train loss = 0.694\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 400/529 [07:43<02:28,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 400: train loss = 0.720\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▍| 500/529 [09:38<00:33,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"train step 500: train loss = 0.721\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 529/529 [10:12<00:00,  1.16s/it]\n","output_type":"stream"},{"name":"stdout","text":"train evaluation\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 506/529 [09:12<00:25,  1.09s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def precision_macro(conf_matrix):\n  #macro = averaged across precisions for each class\n  n_classes = conf_matrix.shape[0]\n  by_class = []\n  for i in range(n_classes):\n    val = conf_matrix[i, i]/conf_matrix[:, i].sum()\n    by_class.append(val)\n  return by_class, sum(by_class)/n_classes # class-wise precision and macro\n\ndef recall_macro(conf_matrix):\n  #macro = averaged across recalls for each class\n  n_classes = conf_matrix.shape[0]\n  by_class = []\n  for i in range(n_classes):\n    val = conf_matrix[i, i]/conf_matrix[i, :].sum()\n    by_class.append(val)\n  return by_class, sum(by_class)/n_classes # class-wise recall and macro\n\ndef precision_ovo(conf_matrix, one_label=0): #one_label -- the label of the class which is opposed to other ones\n  sub_matrix = conf_matrix[:, one_label]\n  denum = conf_matrix.sum() - sub_matrix.sum()\n  num = conf_matrix.sum() - sub_matrix.sum() - conf_matrix[one_label, :].sum() + conf_matrix[one_label, one_label]\n  return num/denum\ndef recall_ovo(conf_matrix, one_label=0):\n  return conf_matrix[one_label, one_label]/conf_matrix[one_label, :].sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:46:48.124702Z","iopub.execute_input":"2025-01-06T14:46:48.125063Z","iopub.status.idle":"2025-01-06T14:46:48.131983Z","shell.execute_reply.started":"2025-01-06T14:46:48.124975Z","shell.execute_reply":"2025-01-06T14:46:48.131091Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def to_python_types(results):\n  output = {}\n  output['training_loss_history'] = results['training_loss_history']\n  for x in ['train_epochs_res', 'test_epochs_res', 'real_test_res']:\n    if x in [y for y in results.keys()]:\n        output[x] = []\n        for hist_log in results[x]:\n          res = {}\n          res['accuracy_ava'] = hist_log['accuracy_ava'].item()\n          if 'loss' in [y for y in hist_log.keys()]:\n            res['loss'] = hist_log['loss']\n          res['accuracy_ovo'] = hist_log['accuracy_ovo'].item()\n          if 'conf_matrix' in [y for y in hist_log.keys()]:\n            res['conf_matrix'] = hist_log['conf_matrix'].tolist()\n          output[x].append(res)\n\n  return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:46:54.103408Z","iopub.execute_input":"2025-01-06T14:46:54.103690Z","iopub.status.idle":"2025-01-06T14:46:54.109191Z","shell.execute_reply.started":"2025-01-06T14:46:54.103668Z","shell.execute_reply":"2025-01-06T14:46:54.108272Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"test_dataset = AnswersDataset(tokenizer, train_test_data, 'test', max_len=max_seq_length)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                        pin_memory=True, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:46:55.622638Z","iopub.execute_input":"2025-01-06T14:46:55.622925Z","iopub.status.idle":"2025-01-06T14:47:50.089910Z","shell.execute_reply.started":"2025-01-06T14:46:55.622902Z","shell.execute_reply":"2025-01-06T14:47:50.089086Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 18019/18019 [00:54<00:00, 331.03it/s]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def evaluation_epoch_test(model, loader):\n  n_correct, n_total = 0, 0\n  n_correct_oo = 0\n  total_loss = 0\n\n  total_conf_matrix = torch.zeros(size=(3, 3))\n\n  with torch.no_grad():\n    for chunk in tqdm(loader):\n      qx, ax, y, qmask, amask = chunk\n      pred = model(qx.to(device), ax.to(device), qmask.to(device), amask.to(device)).cpu()\n      apred = torch.argmax(pred, 1) # for all-vs-all classification\n\n      oopred = torch.where(apred > 0, 1, 0) # for one-vs-others classification\n      ooy = torch.where(y > 0, 1, 0)\n\n      n_correct += (apred == y).sum()\n      n_correct_oo += (oopred == ooy).sum()\n\n      n_total += y.shape[0]\n\n      #total_loss += y.shape[0] * loss_obj(pred, y).item()\n      total_conf_matrix+=conf_matrix(apred, y, num_classes=3)\n\n  return {'accuracy_ava': n_correct/n_total, 'loss': -1,\n          'accuracy_ovo':n_correct_oo/n_total, 'conf_matrix': total_conf_matrix}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:47:50.091073Z","iopub.execute_input":"2025-01-06T14:47:50.091322Z","iopub.status.idle":"2025-01-06T14:47:50.097425Z","shell.execute_reply.started":"2025-01-06T14:47:50.091303Z","shell.execute_reply":"2025-01-06T14:47:50.096552Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"test_results = evaluation_epoch_test(model1, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:47:50.098595Z","iopub.execute_input":"2025-01-06T14:47:50.098889Z","iopub.status.idle":"2025-01-06T14:49:56.850025Z","shell.execute_reply.started":"2025-01-06T14:47:50.098857Z","shell.execute_reply":"2025-01-06T14:49:56.848961Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 141/141 [02:06<00:00,  1.11it/s]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"mtx = test_results['conf_matrix']\nprecision_ovo(mtx).item(), recall_ovo(mtx).item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:50:00.121843Z","iopub.execute_input":"2025-01-06T14:50:00.122189Z","iopub.status.idle":"2025-01-06T14:50:00.133661Z","shell.execute_reply.started":"2025-01-06T14:50:00.122163Z","shell.execute_reply":"2025-01-06T14:50:00.132875Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(0.9650603532791138, 0.9271383285522461)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"mtx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:50:24.113225Z","iopub.execute_input":"2025-01-06T14:50:24.113548Z","iopub.status.idle":"2025-01-06T14:50:24.157319Z","shell.execute_reply.started":"2025-01-06T14:50:24.113521Z","shell.execute_reply":"2025-01-06T14:50:24.156623Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"tensor([[5268.,   68.,  346.],\n        [ 548., 4286., 1827.],\n        [ 352.,  377., 4945.]])"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"precision_macro(mtx), recall_macro(mtx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:50:29.047603Z","iopub.execute_input":"2025-01-06T14:50:29.048068Z","iopub.status.idle":"2025-01-06T14:50:29.056539Z","shell.execute_reply.started":"2025-01-06T14:50:29.048029Z","shell.execute_reply":"2025-01-06T14:50:29.055876Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(([tensor(0.8541), tensor(0.9059), tensor(0.6947)], tensor(0.8182)),\n ([tensor(0.9271), tensor(0.6434), tensor(0.8715)], tensor(0.8140)))"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"train_hist['real_test_res'] = [test_results]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:50:32.327315Z","iopub.execute_input":"2025-01-06T14:50:32.327612Z","iopub.status.idle":"2025-01-06T14:50:32.331424Z","shell.execute_reply.started":"2025-01-06T14:50:32.327590Z","shell.execute_reply":"2025-01-06T14:50:32.330517Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"train_hist['real_test_res']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:50:33.875766Z","iopub.execute_input":"2025-01-06T14:50:33.876112Z","iopub.status.idle":"2025-01-06T14:50:33.883193Z","shell.execute_reply.started":"2025-01-06T14:50:33.876083Z","shell.execute_reply":"2025-01-06T14:50:33.882409Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[{'accuracy_ava': tensor(0.8047),\n  'loss': -1,\n  'accuracy_ovo': tensor(0.9271),\n  'conf_matrix': tensor([[5268.,   68.,  346.],\n          [ 548., 4286., 1827.],\n          [ 352.,  377., 4945.]])}]"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"to_python_types(train_hist)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T22:12:37.052343Z","iopub.execute_input":"2024-12-23T22:12:37.052625Z","iopub.status.idle":"2024-12-23T22:12:37.067848Z","shell.execute_reply.started":"2024-12-23T22:12:37.052604Z","shell.execute_reply":"2024-12-23T22:12:37.067137Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'training_loss_history': [1.0980483293533325,\n  1.0823463201522827,\n  1.067246437072754,\n  1.0368599891662598,\n  0.9960927367210388,\n  0.9941471815109253,\n  0.9989827275276184,\n  0.993592381477356,\n  0.9695610404014587,\n  0.9913089871406555,\n  0.9668630957603455,\n  0.9214809536933899,\n  0.9344689249992371,\n  0.9052056074142456,\n  0.9105445742607117,\n  0.9156138896942139,\n  0.9211647510528564,\n  0.8600334525108337,\n  0.9804888367652893,\n  0.849987804889679,\n  0.9319008588790894,\n  0.9165372252464294,\n  0.9323073625564575,\n  0.9196144342422485,\n  0.8164414167404175,\n  0.911033570766449,\n  0.9009101390838623,\n  0.9408165812492371,\n  0.8721864819526672,\n  0.9039574265480042,\n  0.8663275837898254,\n  0.925107479095459,\n  0.8617335557937622,\n  0.9045649766921997,\n  0.8469796180725098,\n  0.8280636072158813,\n  0.9065492749214172,\n  0.9020992517471313,\n  0.9356536865234375,\n  0.8470385670661926,\n  0.8773105144500732,\n  0.8560386300086975,\n  0.8146309852600098,\n  0.866750955581665,\n  0.8547725677490234,\n  0.807673454284668,\n  0.8838202357292175,\n  0.8204955458641052,\n  0.8518444299697876,\n  0.8630292415618896,\n  0.8274299502372742,\n  0.8887803554534912,\n  0.8289588689804077,\n  0.9062227606773376,\n  0.8241664171218872,\n  0.795258641242981,\n  0.8506209254264832,\n  0.810265839099884,\n  0.7802544236183167,\n  0.8213065266609192,\n  0.7900123596191406,\n  0.805998682975769,\n  0.8747395873069763,\n  0.8944463133811951,\n  0.846792995929718,\n  0.8172264695167542,\n  0.8622569441795349,\n  0.890384316444397,\n  0.8695433735847473,\n  0.8494547009468079,\n  0.8179351687431335,\n  0.8487530946731567,\n  0.8345023393630981,\n  0.8492404222488403,\n  0.803713321685791,\n  0.8230481743812561,\n  0.8459703326225281,\n  0.8399368524551392,\n  0.801276445388794,\n  0.7940609455108643,\n  0.83006352186203,\n  0.833470344543457,\n  0.8226953744888306,\n  0.8043004274368286,\n  0.894117534160614,\n  0.8655754923820496,\n  0.8643829822540283,\n  0.8369823694229126,\n  0.9336891770362854,\n  0.8391230702400208,\n  0.8116226196289062,\n  0.8068773746490479,\n  0.8949254751205444,\n  0.900696873664856,\n  0.8649647831916809,\n  0.8276467323303223,\n  0.8637373447418213,\n  0.9050010442733765,\n  0.8603762984275818,\n  0.8215126395225525,\n  0.8351398706436157,\n  0.8568554520606995,\n  0.7530848979949951,\n  0.8039832711219788,\n  0.8568439483642578,\n  0.8181197643280029,\n  0.8010544776916504,\n  0.7710886001586914,\n  0.8521101474761963,\n  0.8547877669334412,\n  0.8087096810340881,\n  0.7913689017295837,\n  0.8897577524185181,\n  0.8175693154335022,\n  0.9077000021934509,\n  0.8534703254699707,\n  0.837173342704773,\n  0.7880520224571228,\n  0.8208554983139038,\n  0.8354178071022034,\n  0.8277592062950134,\n  0.903831958770752,\n  0.9113302230834961,\n  0.8405863642692566,\n  0.8039587736129761,\n  0.8516892194747925,\n  0.8103331327438354,\n  0.8612340092658997,\n  0.7589786648750305,\n  0.8452498316764832,\n  0.8216069936752319,\n  0.7463769912719727,\n  0.8358224034309387,\n  0.8263388872146606,\n  0.782837986946106,\n  0.7574104070663452,\n  0.7890207767486572,\n  0.8533948659896851,\n  0.8916823267936707,\n  0.8885719776153564,\n  0.7688738107681274,\n  0.7749502658843994,\n  0.8315191268920898,\n  0.8094109296798706,\n  0.8305543065071106,\n  0.8043256998062134,\n  0.8597113490104675,\n  0.8590177297592163,\n  0.7704790830612183,\n  0.8360809087753296,\n  0.8596722483634949,\n  0.8071266412734985,\n  0.7945477962493896,\n  0.783198893070221,\n  0.8134733438491821,\n  0.7913214564323425,\n  0.8072015047073364,\n  0.8236335515975952,\n  0.8373972177505493,\n  0.8090125918388367,\n  0.8683523535728455,\n  0.8170182108879089,\n  0.8216394782066345,\n  0.7917089462280273,\n  0.8547895550727844,\n  0.8104148507118225,\n  0.8821099996566772,\n  0.8708733320236206,\n  0.7873121500015259,\n  0.8223590850830078,\n  0.8347721695899963,\n  0.7955651879310608,\n  0.8422159552574158,\n  0.7662978172302246,\n  0.7947787046432495,\n  0.7710074186325073,\n  0.8465802073478699,\n  0.7923705577850342,\n  0.786071240901947,\n  0.8361010551452637,\n  0.7919058799743652,\n  0.7671121954917908,\n  0.8047940731048584,\n  0.7969815731048584,\n  0.8035631775856018,\n  0.8133280277252197,\n  0.7853744626045227,\n  0.8203868865966797,\n  0.813144862651825,\n  0.8056508898735046,\n  0.8725674748420715,\n  0.835224449634552,\n  0.7435882091522217,\n  0.82918781042099,\n  0.8410683870315552,\n  0.7599877715110779,\n  0.7702688574790955,\n  0.8323869109153748,\n  0.847679615020752,\n  0.7713044881820679,\n  0.8438397645950317,\n  0.8212659358978271,\n  0.8573157787322998,\n  0.7880343198776245,\n  0.8308426141738892,\n  0.820746660232544,\n  0.8246537446975708,\n  0.7992600202560425,\n  0.8175775408744812,\n  0.844368577003479,\n  0.7935069799423218,\n  0.7811332941055298,\n  0.7883881330490112,\n  0.7316921949386597,\n  0.8550311923027039,\n  0.8281666040420532,\n  0.8618373870849609,\n  0.8101795315742493,\n  0.7363235354423523,\n  0.7804184556007385,\n  0.8327288031578064,\n  0.7186837196350098,\n  0.794018566608429,\n  0.8271533250808716,\n  0.7985873222351074,\n  0.8105192184448242,\n  0.7591309547424316,\n  0.7733153104782104,\n  0.8185971975326538,\n  0.7709285616874695,\n  0.7911496162414551,\n  0.775876522064209,\n  0.7644854784011841,\n  0.8477901816368103,\n  0.8231947422027588,\n  0.8206925988197327,\n  0.8120099306106567,\n  0.7676886320114136,\n  0.7947636246681213,\n  0.8725051879882812,\n  0.7944548726081848,\n  0.8107038736343384,\n  0.8631165027618408,\n  0.7622455358505249,\n  0.8524371385574341,\n  0.8036239743232727,\n  0.8106435537338257,\n  0.7731200456619263,\n  0.8539332747459412,\n  0.7796425819396973,\n  0.7885257005691528,\n  0.7778461575508118,\n  0.8252156972885132,\n  0.7816281318664551,\n  0.7663395404815674,\n  0.8371760845184326,\n  0.8125003576278687,\n  0.8066830039024353,\n  0.7997618317604065,\n  0.7766683101654053,\n  0.7905157208442688,\n  0.7882745265960693,\n  0.8032791614532471,\n  0.7520222663879395,\n  0.7946439385414124,\n  0.7979977130889893,\n  0.781396746635437,\n  0.7747021317481995,\n  0.7904279232025146,\n  0.7527555227279663,\n  0.7744051218032837,\n  0.8234468698501587,\n  0.8210382461547852,\n  0.8223629593849182,\n  0.8076955080032349,\n  0.8333848714828491,\n  0.7519283294677734,\n  0.8400449156761169,\n  0.7589802742004395,\n  0.7801320552825928,\n  0.7761993408203125,\n  0.810799241065979,\n  0.763400137424469,\n  0.7482922673225403,\n  0.8665653467178345,\n  0.7998467683792114,\n  0.8075874447822571,\n  0.7529464364051819,\n  0.7950830459594727,\n  0.8471342921257019,\n  0.8300589919090271,\n  0.8655502200126648,\n  0.7291719317436218,\n  0.7963222861289978,\n  0.7815661430358887,\n  0.8055071830749512,\n  0.7657824754714966,\n  0.9038598537445068,\n  0.7730004191398621,\n  0.8147138953208923,\n  0.7888709902763367,\n  0.7891338467597961,\n  0.7968109250068665,\n  0.8336027264595032,\n  0.7339644432067871,\n  0.8351746201515198,\n  0.7270400524139404,\n  0.741824209690094,\n  0.8189908266067505,\n  0.7563816905021667,\n  0.7839873433113098,\n  0.773230254650116,\n  0.7502909302711487,\n  0.7584241628646851,\n  0.7987368702888489,\n  0.7808646559715271,\n  0.765359103679657,\n  0.8132051825523376,\n  0.7676849961280823,\n  0.7057915329933167,\n  0.7940372228622437,\n  0.7795614004135132,\n  0.8548081517219543,\n  0.8499006032943726,\n  0.8812000155448914,\n  0.7839751243591309,\n  0.7816228866577148,\n  0.8241379261016846,\n  0.7690592408180237,\n  0.7768269777297974,\n  0.7904071807861328,\n  0.8033509254455566,\n  0.745995044708252,\n  0.8425632119178772,\n  0.7936447262763977,\n  0.8076290488243103,\n  0.7815263271331787,\n  0.7997321486473083,\n  0.848914623260498,\n  0.752433717250824,\n  0.803165078163147,\n  0.8033134937286377,\n  0.8225569128990173,\n  0.8383219838142395,\n  0.790524423122406,\n  0.7518377304077148,\n  0.8398612141609192,\n  0.8126339912414551,\n  0.7689651250839233,\n  0.8258945941925049,\n  0.7561228275299072,\n  0.8229609727859497,\n  0.8179521560668945,\n  0.8191261887550354,\n  0.7746722102165222,\n  0.8514769673347473,\n  0.7687880992889404,\n  0.8301501870155334,\n  0.7950528860092163,\n  0.7556250691413879,\n  0.829153835773468,\n  0.7961500883102417,\n  0.7520540952682495,\n  0.752144455909729,\n  0.7820619940757751,\n  0.7933163642883301,\n  0.7492884397506714,\n  0.8152710795402527,\n  0.7904259562492371,\n  0.7926190495491028,\n  0.813195526599884,\n  0.8206288814544678,\n  0.8027735352516174,\n  0.7919559478759766,\n  0.7970944046974182,\n  0.791918158531189,\n  0.8254328966140747,\n  0.7732183337211609,\n  0.8221762180328369,\n  0.7216565608978271,\n  0.8354646563529968,\n  0.8199573159217834,\n  0.844359815120697,\n  0.7177951335906982,\n  0.7929874658584595,\n  0.8227090835571289,\n  0.7932144999504089,\n  0.7743434906005859,\n  0.8248645067214966,\n  0.7463743686676025,\n  0.8165121078491211,\n  0.7513021230697632,\n  0.8239107131958008,\n  0.7608640193939209,\n  0.7822900414466858,\n  0.8227091431617737,\n  0.7547080516815186,\n  0.7902873754501343,\n  0.7730202078819275,\n  0.7574850916862488,\n  0.7472378611564636,\n  0.8256761431694031,\n  0.8283597230911255,\n  0.8116784691810608,\n  0.8003939390182495,\n  0.828284502029419,\n  0.7415285110473633,\n  0.7808756828308105,\n  0.8032327890396118,\n  0.8011986017227173,\n  0.7610474824905396,\n  0.747246265411377,\n  0.7480888366699219,\n  0.7884747982025146,\n  0.842076301574707,\n  0.7946968674659729,\n  0.8108241558074951,\n  0.7681196928024292,\n  0.7571988701820374,\n  0.7689947485923767,\n  0.7757353186607361,\n  0.7876354455947876,\n  0.7646414041519165,\n  0.8156775236129761,\n  0.7731783390045166,\n  0.73833829164505,\n  0.7927544116973877,\n  0.8305013179779053,\n  0.8542807102203369,\n  0.8057143092155457,\n  0.7934625744819641,\n  0.6996613144874573,\n  0.7886182069778442,\n  0.760866641998291,\n  0.7803903222084045,\n  0.7876275777816772,\n  0.8427813649177551,\n  0.7314609885215759,\n  0.7703113555908203,\n  0.7897655367851257,\n  0.7459510564804077,\n  0.764501690864563,\n  0.7612595558166504,\n  0.8335213661193848,\n  0.7596126794815063,\n  0.730442464351654,\n  0.7782142758369446,\n  0.7701916694641113,\n  0.7464353442192078,\n  0.7598037719726562,\n  0.7917641997337341,\n  0.7601396441459656,\n  0.7404050827026367,\n  0.7440810203552246,\n  0.7425961494445801,\n  0.7901403307914734,\n  0.8003907203674316,\n  0.7162315249443054,\n  0.8096867203712463,\n  0.7126350998878479,\n  0.8807357549667358,\n  0.7951720356941223,\n  0.7706118226051331,\n  0.8454238772392273,\n  0.8154387474060059,\n  0.8163639903068542,\n  0.7501799464225769,\n  0.7616106271743774,\n  0.7571190595626831,\n  0.7425596714019775,\n  0.7688747644424438,\n  0.7462989091873169,\n  0.7980333566665649,\n  0.7883113622665405,\n  0.8152143359184265,\n  0.7732473015785217,\n  0.779865562915802,\n  0.8172804117202759,\n  0.8276170492172241,\n  0.8013566732406616,\n  0.7551880478858948,\n  0.7875160574913025,\n  0.7679523229598999,\n  0.7274959683418274,\n  0.7170074582099915,\n  0.7653707265853882,\n  0.8420933485031128,\n  0.8348369002342224,\n  0.7485330700874329,\n  0.7947354316711426,\n  0.7776157855987549,\n  0.7558208703994751,\n  0.7598369717597961,\n  0.821816623210907,\n  0.7997868061065674,\n  0.8323054313659668,\n  0.7869797348976135,\n  0.7848709225654602,\n  0.7510728240013123,\n  0.7952318787574768,\n  0.8116483092308044,\n  0.8143252730369568,\n  0.7525993585586548,\n  0.7782224416732788,\n  0.7781603932380676,\n  0.8000506162643433,\n  0.7108882665634155,\n  0.8099461793899536,\n  0.8090894222259521,\n  0.7974323630332947,\n  0.8437902331352234,\n  0.7638967037200928,\n  0.7827732563018799,\n  0.7684409618377686,\n  0.8201014399528503,\n  0.8263542652130127,\n  0.7520403861999512,\n  0.8357633948326111,\n  0.7031115889549255,\n  0.7955405712127686,\n  0.7881726026535034,\n  0.794582724571228,\n  0.8193382024765015,\n  0.8168396949768066,\n  0.74262934923172,\n  0.8294795751571655,\n  0.7739072442054749,\n  0.8197246789932251,\n  0.8021079301834106,\n  0.7320184707641602,\n  0.7414662837982178,\n  0.7566923499107361,\n  0.8039581179618835,\n  0.7581814527511597,\n  0.8036969900131226,\n  0.7984203100204468,\n  0.7881113886833191,\n  0.7635758519172668,\n  0.7750993371009827,\n  0.7507335543632507,\n  0.7595753073692322,\n  0.7860482931137085,\n  0.7106435894966125,\n  0.740801215171814,\n  0.7143710255622864,\n  0.7608732581138611,\n  0.7699737548828125,\n  0.7834215760231018,\n  0.735238790512085,\n  0.6948155760765076,\n  0.800713062286377,\n  0.7731122970581055,\n  0.7558258175849915,\n  0.7618771195411682,\n  0.7761353850364685,\n  0.8576098084449768,\n  0.7432323098182678,\n  0.7420039772987366,\n  0.7789767384529114,\n  0.7303426265716553,\n  0.8046498894691467,\n  0.8103008270263672,\n  0.7604730725288391,\n  0.7969768047332764,\n  0.8474399447441101,\n  0.7525948882102966,\n  0.7615985870361328,\n  0.7832108736038208,\n  0.7999219298362732,\n  0.7522578835487366,\n  0.7557480931282043,\n  0.7502117156982422,\n  0.8050339221954346,\n  0.7335063219070435,\n  0.756226122379303,\n  0.818576991558075,\n  0.7460590600967407,\n  0.7610741257667542,\n  0.7589502334594727,\n  0.7877771258354187,\n  0.8204118609428406,\n  0.815904438495636,\n  0.7803958654403687,\n  0.770817756652832,\n  0.7799841165542603,\n  0.7117128372192383,\n  0.7954670190811157,\n  0.7805233001708984,\n  0.7591890096664429,\n  0.7574692964553833,\n  0.8322053551673889,\n  0.7667877078056335,\n  0.7584072351455688,\n  0.7576795220375061,\n  0.7518823146820068,\n  0.825697124004364,\n  0.7490192651748657,\n  0.8188310861587524,\n  0.7713750600814819,\n  0.7938606142997742,\n  0.7990193367004395,\n  0.7735661864280701,\n  0.7990058660507202,\n  0.7362930178642273,\n  0.8006495833396912,\n  0.700840175151825,\n  0.7987467646598816,\n  0.7659515142440796,\n  0.6880103349685669,\n  0.7839368581771851,\n  0.8151916861534119,\n  0.8066550493240356,\n  0.8267810344696045,\n  0.8040212988853455,\n  0.8151136040687561,\n  0.7630423307418823,\n  0.8335870504379272,\n  0.8037484884262085,\n  0.7823300361633301,\n  0.7397759556770325,\n  0.7555383443832397,\n  0.7570921182632446,\n  0.7775975465774536,\n  0.7599970102310181,\n  0.7477766275405884,\n  0.8233726620674133,\n  0.7998031973838806,\n  0.7076459527015686,\n  0.7397988438606262,\n  0.778101921081543,\n  0.7755810022354126,\n  0.7652505040168762,\n  0.7698352336883545,\n  0.7699829339981079,\n  0.7817183136940002,\n  0.7516125440597534,\n  0.7903006076812744,\n  0.750122606754303,\n  0.7660784125328064,\n  0.7760404944419861,\n  0.7983155846595764,\n  0.7301850318908691,\n  0.7836425304412842,\n  0.7795919179916382,\n  0.8079455494880676,\n  0.8009576201438904,\n  0.7577333450317383,\n  0.7289769649505615,\n  0.749381959438324,\n  0.8149088025093079,\n  0.7708568572998047,\n  0.7302651405334473,\n  0.7523046731948853,\n  0.7525549530982971,\n  0.8047291040420532,\n  0.7824686765670776,\n  0.7403110861778259,\n  0.7817985415458679,\n  0.8138067722320557,\n  0.8115313053131104,\n  0.726068913936615,\n  0.8231489062309265,\n  0.7994434237480164,\n  0.7348510026931763,\n  0.8561651110649109,\n  0.7675366997718811,\n  0.7322508096694946,\n  0.7346529960632324,\n  0.7633153796195984,\n  0.8557997941970825,\n  0.7089090347290039,\n  0.7101107835769653,\n  0.7637690901756287,\n  0.7358693480491638,\n  0.787609875202179,\n  0.7932666540145874,\n  0.8194695115089417,\n  0.7815108299255371,\n  0.7665055394172668,\n  0.7448340058326721,\n  0.7734169960021973,\n  0.7811254858970642,\n  0.7455229759216309,\n  0.8256719708442688,\n  0.7624015212059021,\n  0.7364948987960815,\n  0.8129341006278992,\n  0.7185729146003723,\n  0.8125426769256592,\n  0.7643229365348816,\n  0.7709863781929016,\n  0.7137389779090881,\n  0.7634047269821167,\n  0.7926952242851257,\n  0.7293097972869873,\n  0.8233574628829956,\n  0.7589581608772278,\n  0.7542054653167725,\n  0.759786069393158,\n  0.7918900847434998,\n  0.7535466551780701,\n  0.7743980288505554,\n  0.6970644593238831,\n  0.7498694062232971,\n  0.7279480695724487,\n  0.799760639667511,\n  0.7740518450737,\n  0.7738776206970215,\n  0.7670470476150513,\n  0.8313170075416565,\n  0.741982102394104,\n  0.8060981631278992,\n  0.712436318397522,\n  0.7324264049530029,\n  0.7673601508140564,\n  0.762008547782898,\n  0.8002908825874329,\n  0.7574483156204224,\n  0.7564957141876221,\n  0.7529981732368469,\n  0.7885398864746094,\n  0.7508494257926941,\n  0.8060339689254761,\n  0.7831060290336609,\n  0.7994458675384521,\n  0.7792810797691345,\n  0.7778548002243042,\n  0.7625354528427124,\n  0.769317626953125,\n  0.7513419985771179,\n  0.7903377413749695,\n  0.7159547209739685,\n  0.7568289041519165,\n  0.8158585429191589,\n  0.7939295768737793,\n  0.7799918055534363,\n  0.7853502035140991,\n  0.8158019781112671,\n  0.7889620661735535,\n  0.8000748753547668,\n  0.7792754173278809,\n  0.7352531552314758,\n  0.7700231075286865,\n  0.7924851775169373,\n  0.7515550255775452,\n  0.781760036945343,\n  0.7910112738609314,\n  0.8069063425064087,\n  0.780906617641449,\n  0.8121954202651978,\n  0.7565397620201111,\n  0.7767093181610107,\n  0.7511652112007141,\n  0.7575141787528992,\n  0.7231746912002563,\n  0.7417991161346436,\n  0.7162957191467285,\n  0.8388832807540894,\n  0.7323596477508545,\n  0.7689117789268494,\n  0.7932560443878174,\n  0.7685692310333252,\n  0.7499194145202637,\n  0.758323609828949,\n  0.7675508856773376,\n  0.7955873012542725,\n  0.8155402541160583,\n  0.7500975131988525,\n  0.7763743996620178,\n  0.7621835470199585,\n  0.743695855140686,\n  0.7792118191719055,\n  0.7364224791526794,\n  0.7653746008872986,\n  0.746166467666626,\n  0.810738205909729,\n  0.7281240820884705,\n  0.6702717542648315,\n  0.8252174854278564,\n  0.7960540652275085,\n  0.7663329243659973,\n  0.7221341729164124,\n  0.7546550631523132,\n  0.7351412773132324,\n  0.7307053208351135,\n  0.7703883051872253,\n  0.7450690865516663,\n  0.7995584607124329,\n  0.7672425508499146,\n  0.7326124906539917,\n  0.7758970856666565,\n  0.6990160942077637,\n  0.793136477470398,\n  0.7513845562934875,\n  0.708377480506897,\n  0.7152841091156006,\n  0.7427555322647095,\n  0.712105393409729,\n  0.8025146126747131,\n  0.8278504610061646,\n  0.7550362944602966,\n  0.7480757236480713,\n  0.7687453627586365,\n  0.8474631905555725,\n  0.7212962508201599,\n  0.7005900740623474,\n  0.7300928235054016,\n  0.7604938745498657,\n  0.7488988041877747,\n  0.7739420533180237,\n  0.7841495871543884,\n  0.7503327131271362,\n  0.7398144602775574,\n  0.7215536236763,\n  0.7211886644363403,\n  0.7024607062339783,\n  0.7853804230690002,\n  0.7018828392028809,\n  0.7366100549697876,\n  0.8024502992630005,\n  0.7785468101501465,\n  0.807735025882721,\n  0.7349475622177124,\n  0.7242572903633118,\n  0.7989158034324646,\n  0.7917537689208984,\n  0.7938807010650635,\n  0.8265413641929626,\n  0.7431358098983765,\n  0.7776966691017151,\n  0.6834521889686584,\n  0.7837936878204346,\n  0.6986830234527588,\n  0.7654995918273926,\n  0.7605682611465454,\n  0.8022720813751221,\n  0.7733733057975769,\n  0.7252614498138428,\n  0.7681531310081482,\n  0.7739985585212708,\n  0.6727391481399536,\n  0.7645432353019714,\n  0.762224018573761,\n  0.752973735332489,\n  0.7138352990150452,\n  0.6968942880630493,\n  0.6815267205238342,\n  0.7511571049690247,\n  0.7096280455589294,\n  0.7313725352287292,\n  0.7575443387031555,\n  0.6480998992919922,\n  0.7566778659820557,\n  0.7360889315605164,\n  0.7599236369132996,\n  0.7698115706443787,\n  0.7871398329734802,\n  0.7655830383300781,\n  0.8059496283531189,\n  0.7745646834373474,\n  0.7400252819061279,\n  0.7284404039382935,\n  0.7653332352638245,\n  0.7757751941680908,\n  0.7482011318206787,\n  0.74260014295578,\n  0.7859452962875366,\n  0.8191234469413757,\n  0.8622854351997375,\n  0.7614693641662598,\n  0.776602029800415,\n  0.7878125905990601,\n  0.7997117638587952,\n  0.7664768099784851,\n  0.6453189849853516,\n  0.7405655384063721,\n  0.7489262819290161,\n  0.7435736060142517,\n  0.7257885932922363,\n  0.7547608613967896,\n  0.7482227683067322,\n  0.749889612197876,\n  0.7379533648490906,\n  0.7887172102928162,\n  0.7572900652885437,\n  0.7332673668861389,\n  0.7963205575942993,\n  0.7420868277549744,\n  0.7729450464248657,\n  0.6866099834442139,\n  0.7825456857681274,\n  0.8040522933006287,\n  0.752122163772583,\n  0.7684555649757385,\n  0.720605731010437,\n  0.7697548270225525,\n  0.722169816493988,\n  0.8250964879989624,\n  0.7700430750846863,\n  0.7570428848266602,\n  0.7375367283821106,\n  0.8061347603797913,\n  0.7488868832588196,\n  0.7630590796470642,\n  0.7862875461578369,\n  0.8259190320968628,\n  0.7545679211616516,\n  0.7562047243118286,\n  0.7042755484580994,\n  0.7480369806289673,\n  0.7615166902542114,\n  0.7627880573272705,\n  0.7910178899765015,\n  0.7686896324157715,\n  0.7542778253555298,\n  0.7426683902740479,\n  0.7581016421318054,\n  0.8261314630508423,\n  0.6996421813964844,\n  0.7600823044776917,\n  0.8222228288650513,\n  0.7621690630912781,\n  0.714362621307373,\n  0.710942804813385,\n  0.7658279538154602,\n  0.7878923416137695,\n  0.7162694334983826,\n  0.7746731042861938,\n  0.7541808485984802,\n  0.8046131730079651,\n  0.79701167345047,\n  0.8240904808044434,\n  0.8201439380645752,\n  0.7187933921813965,\n  0.731793224811554,\n  0.7482532858848572,\n  0.7598303556442261,\n  0.7627875804901123,\n  0.7720319032669067,\n  0.759141206741333,\n  0.7588918209075928,\n  0.7955961227416992,\n  0.7158047556877136,\n  0.7982911467552185,\n  0.786490797996521,\n  0.7213508486747742,\n  0.7454928159713745,\n  0.711927056312561,\n  0.7291507124900818,\n  0.7683857083320618,\n  0.755987286567688,\n  0.698799729347229,\n  0.7924039959907532,\n  0.7311098575592041,\n  0.7821971774101257,\n  0.666837215423584,\n  0.7270921468734741,\n  0.8116808533668518,\n  0.7047756910324097,\n  0.8040124773979187,\n  0.8019993901252747,\n  0.7720282673835754,\n  0.7439984679222107,\n  0.7699661254882812,\n  0.7691245079040527,\n  0.7659888863563538,\n  0.7816725373268127,\n  0.759303867816925,\n  0.8585391044616699,\n  0.8329598307609558,\n  0.8333965539932251,\n  0.7925408482551575,\n  0.7996380925178528,\n  0.7611711621284485,\n  0.739720344543457,\n  0.7213654518127441,\n  0.8004332184791565,\n  0.787972092628479,\n  0.7199437618255615,\n  0.7858593463897705,\n  0.8072146773338318,\n  0.7089429497718811,\n  0.699995756149292,\n  0.7839874029159546,\n  0.7449869513511658,\n  0.7942367792129517,\n  0.7957527041435242,\n  0.75506591796875,\n  0.757267951965332,\n  0.7495650053024292,\n  0.7990022301673889,\n  0.7935670614242554,\n  0.7061140537261963,\n  0.7534720301628113,\n  0.7463416457176208,\n  0.7749407291412354,\n  0.7768694162368774,\n  0.7610263824462891,\n  0.7705121636390686,\n  0.8070907592773438,\n  ...],\n 'train_epochs_res': [{'accuracy_ava': 0.7788184285163879,\n   'loss': 0.7663203862998963,\n   'accuracy_ovo': 0.9011094570159912,\n   'conf_matrix': [[21776.0, 822.0, 898.0],\n    [2202.0, 14386.0, 4168.0],\n    [2772.0, 4110.0, 16557.0]]},\n  {'accuracy_ava': 0.7890266180038452,\n   'loss': 0.7568929154599822,\n   'accuracy_ovo': 0.9127801060676575,\n   'conf_matrix': [[20372.0, 825.0, 2299.0],\n    [1403.0, 13578.0, 5775.0],\n    [1377.0, 2602.0, 19460.0]]},\n  {'accuracy_ava': 0.8105508685112,\n   'loss': 0.736171012481879,\n   'accuracy_ovo': 0.9248349070549011,\n   'conf_matrix': [[21720.0, 568.0, 1208.0],\n    [1576.0, 13662.0, 5518.0],\n    [1736.0, 2218.0, 19485.0]]},\n  {'accuracy_ava': 0.8131952285766602,\n   'loss': 0.7328689499699559,\n   'accuracy_ovo': 0.9252633452415466,\n   'conf_matrix': [[20818.0, 1339.0, 1339.0],\n    [1105.0, 17339.0, 2312.0],\n    [1276.0, 5274.0, 16889.0]]},\n  {'accuracy_ava': 0.820611298084259,\n   'loss': 0.7266834555143014,\n   'accuracy_ovo': 0.9310100078582764,\n   'conf_matrix': [[21822.0, 491.0, 1183.0],\n    [1424.0, 13330.0, 6002.0],\n    [1572.0, 1471.0, 20396.0]]},\n  {'accuracy_ava': 0.8382059931755066,\n   'loss': 0.7090712885917492,\n   'accuracy_ovo': 0.9339055418968201,\n   'conf_matrix': [[21975.0, 698.0, 823.0],\n    [1354.0, 16254.0, 3148.0],\n    [1599.0, 3330.0, 18510.0]]},\n  {'accuracy_ava': 0.8362854719161987,\n   'loss': 0.7115261062434424,\n   'accuracy_ovo': 0.9317929744720459,\n   'conf_matrix': [[22495.0, 339.0, 662.0],\n    [1743.0, 13962.0, 5051.0],\n    [1873.0, 1414.0, 20152.0]]},\n  {'accuracy_ava': 0.8449572324752808,\n   'loss': 0.7028813954971669,\n   'accuracy_ovo': 0.9356635212898254,\n   'conf_matrix': [[21616.0, 538.0, 1342.0],\n    [1209.0, 15493.0, 4054.0],\n    [1266.0, 2086.0, 20087.0]]}],\n 'test_epochs_res': [{'accuracy_ava': 0.7625641822814941,\n   'loss': 0.7818906485885599,\n   'accuracy_ovo': 0.8972257375717163,\n   'conf_matrix': [[5998.0, 243.0, 273.0],\n    [875.0, 5637.0, 1694.0],\n    [791.0, 1165.0, 4555.0]]},\n  {'accuracy_ava': 0.7667561769485474,\n   'loss': 0.7778010634070857,\n   'accuracy_ovo': 0.9137110710144043,\n   'conf_matrix': [[5645.0, 220.0, 649.0],\n    [559.0, 5294.0, 2353.0],\n    [404.0, 767.0, 5340.0]]},\n  {'accuracy_ava': 0.7852197289466858,\n   'loss': 0.7605584837276248,\n   'accuracy_ovo': 0.9221421480178833,\n   'conf_matrix': [[5996.0, 167.0, 351.0],\n    [618.0, 5348.0, 2240.0],\n    [517.0, 667.0, 5327.0]]},\n  {'accuracy_ava': 0.8074984550476074,\n   'loss': 0.7389550248209124,\n   'accuracy_ovo': 0.9256276488304138,\n   'conf_matrix': [[5744.0, 369.0, 401.0],\n    [432.0, 6803.0, 971.0],\n    [377.0, 1537.0, 4597.0]]},\n  {'accuracy_ava': 0.788846492767334,\n   'loss': 0.7567717083478231,\n   'accuracy_ovo': 0.9265696406364441,\n   'conf_matrix': [[6011.0, 138.0, 365.0],\n    [587.0, 5159.0, 2460.0],\n    [469.0, 464.0, 5578.0]]},\n  {'accuracy_ava': 0.8183316588401794,\n   'loss': 0.7286279256701733,\n   'accuracy_ovo': 0.9291601777076721,\n   'conf_matrix': [[6031.0, 209.0, 274.0],\n    [537.0, 6327.0, 1342.0],\n    [484.0, 1011.0, 5016.0]]},\n  {'accuracy_ava': 0.8050963282585144,\n   'loss': 0.7424016469113708,\n   'accuracy_ovo': 0.9267109632492065,\n   'conf_matrix': [[6189.0, 97.0, 228.0],\n    [677.0, 5394.0, 2135.0],\n    [554.0, 447.0, 5510.0]]},\n  {'accuracy_ava': 0.8204041123390198,\n   'loss': 0.7269143889921833,\n   'accuracy_ovo': 0.9324572682380676,\n   'conf_matrix': [[5932.0, 158.0, 424.0],\n    [478.0, 6016.0, 1712.0],\n    [374.0, 667.0, 5470.0]]}],\n 'real_test_res': [{'accuracy_ava': 0.8231670260429382,\n   'loss': -1,\n   'accuracy_ovo': 0.9331742525100708,\n   'conf_matrix': [[5202.0, 137.0, 343.0],\n    [414.0, 4836.0, 1411.0],\n    [310.0, 571.0, 4793.0]]}]}"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"import json\nwith open('distilbert_train_val_res_v4.json', 'w') as f:\n    json.dump(to_python_types(train_hist), f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:50:37.716543Z","iopub.execute_input":"2025-01-06T14:50:37.716825Z","iopub.status.idle":"2025-01-06T14:50:37.727760Z","shell.execute_reply.started":"2025-01-06T14:50:37.716804Z","shell.execute_reply":"2025-01-06T14:50:37.727090Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}